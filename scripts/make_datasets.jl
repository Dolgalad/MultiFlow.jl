"""
Script for generating training and testing datasets. For each base instance in `instances/sndlib` create a 10000 instance training set and 1000 test instances. Each instance directory created by this script contains : 
  - `link.csv, service.csv` : instance files
  - `sol.jld2` : a JLD2 data file containing the [`MCFSolution`](@ref) of the instance
  - `stats.json` : a JSON file containing solver run statistics
  - `labels.jld2` : a JLD2 data file containing the labels for the instance

Dataset types : 
  - `cgcolumns` : each instance is solved by column generation and for each demand `k` all arcs belonging to at least one column generated by the solver is labeled 1. All instances have the same number of demands as the original problem, demand endpoints are randomly sampled from original set of endpoints as well as demand amounts
  - `cgcolumns_rnk0.1` : same as `cgcolumns` but with randomized number of demands (``|K| \\pm |K|/10``).
"""

using MultiFlows
using JSON
using JLD2
using FileIO
using Random
using BenchmarkTools

Random.seed!(2023) # for reproducability

base_instance_dir = "./instances/sndlib"
dataset_dir = "./instances/datasets"
mkpath(dataset_dir)

n_train = 100
n_test = 10

overwrite = true # if dataset creation was interrupted setting this to true can speed things up, but there is no guarantee to produce same instances as was used in our tests

"""
Solve and labeling callback for the `cgcolumns` dataset type.
"""
function cgcolumns(instance_path)
    pb = MultiFlows.load(instance_path)
    # solve by column generation and retrieve the RMP model
    sol, (rmp, stats) = solve_column_generation(pb, return_rmp=true)
    # save solution
    solution_path = joinpath(instance_path, "sol.jld2")
    FileIO.save(sol, solution_path)
    # save solver statistics
    stats_path = joinpath(instance_path, "stats.json")
    open(stats_path, "w") do f
        JSON.print(f, stats, 4)
    end
    # create the labels
    labels = zeros(Bool, nk(pb), ne(pb))
    for k in 1:nk(pb)
        labels[k, vcat(rmp.columns[k]...)] .= 1
    end
    labels_path = joinpath(instance_path, "labels.jld2")
    jldsave(labels_path; labels)
end

dataset_types = Dict(
                     "cgcolumns" => (generate_example,cgcolumns),
                     "cgcolumns_rnk0.1" =>(
                                           pb->generate_example(pb, nK=nk(pb)+trunc(Int64, (rand() - 0.5) * 0.2 * nk(pb) )),
                                           cgcolumns
                                       ),
                    )

# dummy run to ensure time measurements are accurate
pb = load("./instances/sndlib/Oxford_0_1_1", edge_dir=:double)
for v in dataset_types
    make_dataset(pb, 1, "dummy", perturbation_f=v.second[1], labeling_f=v.second[2])
end
rm("dummy", recursive=true, force=true)

for base_instance_path in readdir(base_instance_dir, join=true)
    if !is_instance_dir(base_instance_path)
        continue
    end
    base_instance_name = basename(base_instance_path)
    pb = MultiFlows.load(base_instance_path, edge_dir=:double)
    # iterate over dataset types
    for v in dataset_types
        # train dataset
        train_dataset_name = join([base_instance_name, v.first, "train"], "_")
        train_dataset_path = joinpath(dataset_dir, train_dataset_name)
        make_dataset(pb, n_train, train_dataset_path, 
                     progress_prefix="$(train_dataset_name): ", 
                     overwrite=overwrite,
                     perturbation_f=v.second[1],
                     labeling_f=v.second[2], 
                    )
        # test dataset
        test_dataset_name = join([base_instance_name, v.first, "test"], "_")
        test_dataset_path = joinpath(dataset_dir, test_dataset_name)
        make_dataset(pb, n_train, test_dataset_path, 
                     progress_prefix="$(test_dataset_name): ", 
                     overwrite=overwrite,
                     perturbation_f=v.second[1],
                     labeling_f=v.second[2], 

                    )
    end
end
